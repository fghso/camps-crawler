# -*- coding: iso-8859-1 -*-

import socket
import time
import common


class Crawler:
    # Upon initialization the crawler object receives a copy of everything in the client 
    # section of the XML configuration file as the parameter configurationsDictionary
    def __init__(self, configurationsDictionary):
        self.config = configurationsDictionary

    # This is the method that effectively does the collection. The method receives as arguments the ID of the resource to 
    # be collected and all data (if any) generated by the filters added to server. The return value must be a tuple  
    # containing a resource information dictionary at the first position. This information is user defined and must be 
    # understood by the persistence handler used. Aditional information can be sent as the second element of the tuple. 
    # This information is passed to all filters via callback method and is not used by the server. If feedback option is
    # enabled, the resources to be stored can be given in a list as the third element of the tuple. Each new resource is
    # described by a tuple, where the first element is the resource ID and the second element is a dictionary containing
    # resource information (also in a format understood by the persistence handler used)
    def crawl(self, resourceID, filters):
        echo = common.EchoHandler(self.config)

        echo.out("Resource received: %s" % resourceID)
        
        sleepTime = 30
        echo.out("Sleeping for %d seconds..." % sleepTime)
        time.sleep(sleepTime)
        echo.out("Awaked!\n")
        
        newResources = []
        newResources.append((resourceID + 1, {"crawler_name": "c1", "response_code": 3}))
        newResources.append((resourceID + 2, {"crawler_name": "c2", "response_code": 4}))
        extraInfo = {"savecsv": newResources[:1], "savejson": newResources[1:]}
        
        return ({"crawler_name": socket.gethostname(), "response_code": 4}, extraInfo, newResources)
            